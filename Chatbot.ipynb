{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8e9995e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2c3bb4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key= os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6b823237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 1\n",
      "\n",
      "--- Document preview ---\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Attention Is All You Need\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 Introduction\n",
      "2 Background\n",
      "\n",
      "3 Model Architecture\n",
      "\n",
      "\n",
      "3.1 Encoder and Decoder Stacks\n",
      "\n",
      "Encoder:\n",
      "Decoder:\n",
      "\n",
      "\n",
      "\n",
      "3.2 Attention\n",
      "\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "3.2.2 Multi-Head Attention\n",
      "3.2.3 Applications of Attention in our Model\n",
      "\n",
      "\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "3.4 Embeddings and Softmax\n",
      "3.5 Positional Encoding\n",
      "\n",
      "\n",
      "4 Why Self-Attention\n",
      "\n",
      "5 Training\n",
      "\n",
      "5.1 Training Data and Batching\n",
      "5.2 Hardware and Schedule\n",
      "5.3 Optimizer\n",
      "\n",
      "5.4 Regularization\n",
      "\n",
      "Residual Dropout\n",
      "Label Smoothing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6 Results\n",
      "\n",
      "6.1 Machine Translation\n",
      "6.2 Model Variations\n",
      "6.3 English Constituency Parsing\n",
      "\n",
      "\n",
      "\n",
      "7 Conclusion\n",
      "\n",
      "Acknowledgements\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n",
      "\n",
      "Attention Is All You Need\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "Ashish Vaswani\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "&Noam Shazeer11footnotemark: 1\n",
      "Google Brain\n",
      "noam@google.com\n",
      "&Niki Parmar11footnotemark: 1\n",
      "Google Research\n",
      "nikip@google.com\n",
      "&Jakob Uszkoreit11footnotemark: 1\n",
      "Google Research\n",
      "usz@google.com\n",
      "&Llion Jones11footnotemark: 1\n",
      "Google Research\n",
      "llion@google.com\n",
      "&Aidan N. Gomez11footnotemark: 1   \n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "&Łukasz Kaiser11footnotemark: 1\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "&Illia Polosukhin11footnotemark: 1   \n",
      "illia.polosukhin@gmail.com\n",
      "\n",
      "Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Document \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "URL = \"https://arxiv.org/html/1706.03762\"\n",
    "\n",
    "loader = WebBaseLoader(URL)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(documents)}\")\n",
    "print(\"\\n--- Document preview ---\\n\")\n",
    "print(documents[0].page_content[:1500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "aeda5cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 56\n",
      "\n",
      "--- First chunk preview ---\n",
      "\n",
      "Attention Is All You Need\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 Introduction\n",
      "2 Background\n",
      "\n",
      "3 Model Architecture\n",
      "\n",
      "\n",
      "3.1 Encoder and Decoder Stacks\n",
      "\n",
      "Encoder:\n",
      "Decoder:\n",
      "\n",
      "\n",
      "\n",
      "3.2 Attention\n",
      "\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "3.2.2 Multi-Head Attention\n",
      "3.2.3 Applications of Attention in our Model\n",
      "\n",
      "\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "3.4 Embeddings and Softmax\n",
      "3.5 Positional Encoding\n",
      "\n",
      "\n",
      "4 Why Self-Attention\n",
      "\n",
      "5 Training\n",
      "\n",
      "5.1 Training Data and Batching\n",
      "5.2 Hardware and Schedule\n",
      "5.3 Optimizer\n",
      "\n",
      "5.4 Regularization\n",
      "\n",
      "Residual Dropout\n",
      "Label Smoothing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6 Results\n",
      "\n",
      "6.1 Machine Translation\n",
      "6.2 Model Variations\n",
      "6.3 English Constituency Parsing\n",
      "\n",
      "\n",
      "\n",
      "7 Conclusion\n",
      "\n",
      "Acknowledgements\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n",
      "\n",
      "Attention Is All You Need\n"
     ]
    }
   ],
   "source": [
    "# Chunking\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter= RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "chunks= text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(\"\\n--- First chunk preview ---\\n\")\n",
    "print(chunks[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f598df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully\n",
      "280\n"
     ]
    }
   ],
   "source": [
    "# Embeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embedding_model= HuggingFaceEmbeddings (model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks,embedding=embedding_model)\n",
    "\n",
    "print(\"Vector store created successfully\")\n",
    "print(vectorstore._collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3f79117c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Attention Is All You Need', 'language': 'en', 'source': 'https://arxiv.org/html/1706.03762'}, page_content='0.2\\n\\n\\n4.95\\n25.5\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n0.0\\n\\n4.67\\n25.3\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n0.2\\n\\n5.47\\n25.7\\n\\n\\n\\n\\n\\n(E)\\n\\npositional embedding instead of sinusoids\\n\\n4.92\\n25.7\\n\\n\\n\\n\\n\\nbig\\n6\\n1024\\n4096\\n16\\n\\n\\n0.3\\n\\n300K\\n4.33\\n26.4\\n213\\n\\n\\n\\n\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table\\xa03.\\n\\n\\nIn Table\\xa03 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.'),\n",
       " Document(metadata={'language': 'en', 'title': 'Attention Is All You Need', 'source': 'https://arxiv.org/html/1706.03762'}, page_content='0.2\\n\\n\\n4.95\\n25.5\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n0.0\\n\\n4.67\\n25.3\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n0.2\\n\\n5.47\\n25.7\\n\\n\\n\\n\\n\\n(E)\\n\\npositional embedding instead of sinusoids\\n\\n4.92\\n25.7\\n\\n\\n\\n\\n\\nbig\\n6\\n1024\\n4096\\n16\\n\\n\\n0.3\\n\\n300K\\n4.33\\n26.4\\n213\\n\\n\\n\\n\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table\\xa03.\\n\\n\\nIn Table\\xa03 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.'),\n",
       " Document(metadata={'language': 'en', 'source': 'https://arxiv.org/html/1706.03762', 'title': 'Attention Is All You Need'}, page_content='0.2\\n\\n\\n4.95\\n25.5\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n0.0\\n\\n4.67\\n25.3\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n0.2\\n\\n5.47\\n25.7\\n\\n\\n\\n\\n\\n(E)\\n\\npositional embedding instead of sinusoids\\n\\n4.92\\n25.7\\n\\n\\n\\n\\n\\nbig\\n6\\n1024\\n4096\\n16\\n\\n\\n0.3\\n\\n300K\\n4.33\\n26.4\\n213\\n\\n\\n\\n\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table\\xa03.\\n\\n\\nIn Table\\xa03 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.')]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating retriever\n",
    "\n",
    "retriever= vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
    "\n",
    "# Test retriever\n",
    "retriever.invoke(\"What problem does Transformer architecture solve?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "656503b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm= ChatGroq(model=\"openai/gpt-oss-20b\", groq_api_key= groq_api_key, temperature= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5b02aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt \n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful AI assistant. \"\n",
    "            \"Answer the question using ONLY the provided context. \"\n",
    "            \"Cite sources in your answer using [Source 1], [Source 2], etc. \"\n",
    "            \"If the answer is not in the context, say you don't know.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(\"history\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Context:\\n{context}\\n\\nQuestion:\\n{question}\"\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9d3239ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    formatted=[]\n",
    "    for i, doc in enumerate(docs,1):\n",
    "        formatted.append(f\"[source {i}\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3f70f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Chain\n",
    "'''Any normal Python function can be used in a LangChain chain if and only if it is wrapped as a Runnable\n",
    "(usually with RunnableLambda) and correctly returns an output.'''\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from operator import itemgetter\n",
    "\n",
    "rag_chain=(\n",
    "    {\n",
    "        \"docs\": itemgetter(\"question\")|retriever, # ans from retriever wrt to question asked        \n",
    "        \"question\": itemgetter(\"question\"),                                       \n",
    "        \"history\":itemgetter(\"history\")\n",
    "    }\n",
    "    | RunnableLambda(lambda x:{\n",
    "        \"question\": x[\"question\"],\n",
    "        \"history\": x[\"history\"],\n",
    "        \"context\": format_docs(x[\"docs\"]),\n",
    "        \"docs\": x[\"docs\"],\n",
    "    })\n",
    "    |RunnableLambda(lambda x:{\n",
    "        \"answer\":(\n",
    "             prompt\n",
    "            |llm\n",
    "            |StrOutputParser()\n",
    "        ).invoke({\n",
    "            \"question\": x[\"question\"],\n",
    "            \"history\": x[\"history\"],\n",
    "            \"context\": x[\"context\"],\n",
    "        }),\n",
    "        \"sources\": x[\"docs\"],\n",
    "    })\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bf44ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "#in-memory store\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "#Wrap the rag chain with message history\n",
    "conversational_rag= RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history, \n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    "    output_messages_key=\"answer\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "95173c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sources(question, k=3):\n",
    "    doc = retriever.invoke(question)\n",
    "    for i, doc in enumerate(docs[:k], 1):\n",
    "        print(f\"[Source {i}]\")\n",
    "        print(doc.page_content[:600])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5104d20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      "\n",
      "Transformers were introduced to improve sequence‑to‑sequence tasks such as machine translation. In the work cited, the authors evaluate the model on **English‑to‑German translation** using the newstest2013 development set, showing how different architectural choices affect BLEU scores on this translation task. Thus, the primary problem that Transformers solve in this context is **accurate and efficient translation between languages**. [Source 1]\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "\n",
      "[Source 1]\n",
      "0.2\n",
      "\n",
      "\n",
      "4.95\n",
      "25.5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.0\n",
      "\n",
      "4.67\n",
      "25.3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.2\n",
      "\n",
      "5.47\n",
      "25.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(E)\n",
      "\n",
      "positional embedding instead of sinusoids\n",
      "\n",
      "4.92\n",
      "25.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "big\n",
      "6\n",
      "1024\n",
      "4096\n",
      "16\n",
      "\n",
      "\n",
      "0.3\n",
      "\n",
      "300K\n",
      "4.33\n",
      "26.4\n",
      "213\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We pres\n",
      "\n",
      "[Source 2]\n",
      "0.2\n",
      "\n",
      "\n",
      "4.95\n",
      "25.5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.0\n",
      "\n",
      "4.67\n",
      "25.3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.2\n",
      "\n",
      "5.47\n",
      "25.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(E)\n",
      "\n",
      "positional embedding instead of sinusoids\n",
      "\n",
      "4.92\n",
      "25.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "big\n",
      "6\n",
      "1024\n",
      "4096\n",
      "16\n",
      "\n",
      "\n",
      "0.3\n",
      "\n",
      "300K\n",
      "4.33\n",
      "26.4\n",
      "213\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We pres\n",
      "\n",
      "[Source 3]\n",
      "0.2\n",
      "\n",
      "\n",
      "4.95\n",
      "25.5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.0\n",
      "\n",
      "4.67\n",
      "25.3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.2\n",
      "\n",
      "5.47\n",
      "25.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(E)\n",
      "\n",
      "positional embedding instead of sinusoids\n",
      "\n",
      "4.92\n",
      "25.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "big\n",
      "6\n",
      "1024\n",
      "4096\n",
      "16\n",
      "\n",
      "\n",
      "0.3\n",
      "\n",
      "300K\n",
      "4.33\n",
      "26.4\n",
      "213\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We pres\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    " \n",
    "result = conversational_rag.invoke(\n",
    "    {\"question\": \"What problem does transformers solve?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"demo-session\"}}\n",
    ")\n",
    "\n",
    "print(\"ANSWER:\\n\")\n",
    "print(result[\"answer\"])\n",
    "\n",
    "print(\"\\nSOURCES:\\n\")\n",
    "for i, doc in enumerate(result[\"sources\"], 1):\n",
    "    print(f\"\\n[Source {i}]\")\n",
    "    print(doc.page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9007ac0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      "\n",
      "The Transformer’s key advantage over traditional RNN‑based sequence‑to‑sequence models is that it replaces the recurrent layers with **multi‑headed self‑attention**. This change allows the model to capture dependencies across the entire input sequence in parallel, rather than processing tokens one after another as RNNs do. As a result, the Transformer achieves higher accuracy—outperforming the BerkeleyParser even when trained only on a modest 40 k‑sentence WSJ set—and does so without requiring task‑specific tuning [Source 1][Source 2][Source 3].\n",
      "\n",
      "SOURCES:\n",
      "\n",
      "\n",
      "[Source 1]\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n",
      "\n",
      "\n",
      "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7 Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduct\n",
      "\n",
      "[Source 2]\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n",
      "\n",
      "\n",
      "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7 Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduct\n",
      "\n",
      "[Source 3]\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n",
      "\n",
      "\n",
      "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7 Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduct\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    " \n",
    "result = conversational_rag.invoke(\n",
    "    {\"question\": \"Why is this approach better than RNN?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"demo-session\"}}\n",
    ")\n",
    "\n",
    "print(\"ANSWER:\\n\")\n",
    "print(result[\"answer\"])\n",
    "\n",
    "print(\"\\nSOURCES:\\n\")\n",
    "for i, doc in enumerate(result[\"sources\"], 1):\n",
    "    print(f\"\\n[Source {i}]\")\n",
    "    print(doc.page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0acefa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
