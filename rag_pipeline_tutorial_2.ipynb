{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline Tutorial 2\n",
    "\n",
    "This notebook implements a simpler RAG evaluation approach:\n",
    "- **No chunking**: Each context is stored as a single chunk\n",
    "- **ID-based retrieval evaluation**: Checks if the golden context ID appears in top 3 retrieved chunk IDs\n",
    "- Since there's no chunking, each context has a unique ID that matches between golden context and retrieved chunks\n",
    "\n",
    "## Evaluation Approach\n",
    "- Golden context ID = unique identifier for each context\n",
    "- Retrieval Recall@3 = % of questions where golden context ID is found in top 3 retrieved chunk IDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Imports successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Add parent directory to path to import shared modules\n",
    "import pathlib\n",
    "notebook_dir = pathlib.Path().resolve()\n",
    "parent_dir = notebook_dir.parent\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "print(\" Imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Ollama URL: http://localhost:11434\n",
      "  LLM Model: llama3:8b\n",
      "  Embedding Model: BAAI/bge-base-en-v1.5\n",
      "  Device: cuda\n",
      "  K Retrieved: 3\n"
     ]
    }
   ],
   "source": [
    "from config_local import (\n",
    "    OLLAMA_BASE_URL,\n",
    "    LLM_MODEL,\n",
    "    EMBEDDING_MODEL,\n",
    "    DEVICE,\n",
    "    PERSIST_DIRECTORY,\n",
    "    K_RETRIEVED,\n",
    "    TEMPERATURE\n",
    ")\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Ollama URL: {OLLAMA_BASE_URL}\")\n",
    "print(f\"  LLM Model: {LLM_MODEL}\")\n",
    "print(f\"  Embedding Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  K Retrieved: {K_RETRIEVED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check Ollama Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ollama is running\n",
      " Model 'llama3:8b' is available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def check_ollama():\n",
    "    \"\"\"Check if Ollama is running and model is available\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get(\"models\", [])\n",
    "            model_names = [m.get(\"name\", \"\") for m in models]\n",
    "            if LLM_MODEL in model_names:\n",
    "                print(f\" Ollama is running\")\n",
    "                print(f\" Model '{LLM_MODEL}' is available\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\" Model '{LLM_MODEL}' not found\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\" Ollama returned error: {response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\" Cannot connect to Ollama: {e}\")\n",
    "        return False\n",
    "\n",
    "check_ollama()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SQuAD v2 dataset...\n",
      "\n",
      " Loaded 11873 examples\n",
      "\n",
      "Example structure:\n",
      "  ID (unique index): 56ddde6b9a695914005b9628\n",
      "  Title: Normans\n",
      "  Question: In what country is Normandy located?\n",
      "  Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "  Answers: ['France', 'France', 'France', 'France']\n",
      "  Is Impossible: False\n"
     ]
    }
   ],
   "source": [
    "from dataset_loader import load_squad_v2\n",
    "\n",
    "print(\"Loading SQuAD v2 dataset...\")\n",
    "examples = load_squad_v2(split='validation')\n",
    "\n",
    "print(f\"\\n Loaded {len(examples)} examples\")\n",
    "\n",
    "# Examine one example\n",
    "example = examples[0]\n",
    "print(\"\\nExample structure:\")\n",
    "print(f\"  ID (unique index): {example['id']}\")\n",
    "print(f\"  Title: {example['title']}\")\n",
    "print(f\"  Question: {example['question']}\")\n",
    "print(f\"  Context: {example['context'][:200]}...\")\n",
    "print(f\"  Answers: {example['answers']['text']}\")\n",
    "print(f\"  Is Impossible: {example['is_impossible']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Documents WITHOUT Chunking\n",
    "\n",
    "**Key difference**: Each context becomes ONE document chunk. No splitting is performed.\n",
    "Each context's unique ID (from `example['id']`) will be stored in metadata and used for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing documents...\n",
      "Deduplicating contexts to ensure each unique context has only ONE ID\n",
      "\n",
      " Found 1204 unique contexts\n",
      "  Total questions: 11873\n",
      "  Deduplication: 10669 duplicate contexts removed\n",
      "\n",
      " Created 1204 documents (one per unique context)\n",
      "  No chunking performed - each context is a complete chunk\n",
      "\n",
      "Example document:\n",
      "  Content length: 742 characters\n",
      "  Unique Context ID: context_1\n",
      "\n",
      "Example mapping:\n",
      "  Question ID '56ddde6b9a695914005b9628' -> Context ID 'context_1'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from collections import OrderedDict\n",
    "\n",
    "print(\"Preparing documents...\")\n",
    "print(\"Deduplicating contexts to ensure each unique context has only ONE ID\")\n",
    "\n",
    "# Step 1: Deduplicate contexts and assign unique context IDs\n",
    "# Use OrderedDict to preserve first occurrence order\n",
    "unique_contexts = OrderedDict()  # Maps context_text -> unique_context_id\n",
    "question_to_context_id = {}  # Maps question_id -> context_id\n",
    "\n",
    "context_counter = 0\n",
    "\n",
    "for ex in examples:\n",
    "    context_text = ex['context']\n",
    "    question_id = ex['id']\n",
    "    \n",
    "    # If we've seen this context before, reuse its ID\n",
    "    if context_text in unique_contexts:\n",
    "        context_id = unique_contexts[context_text]\n",
    "    else:\n",
    "        # New unique context - assign a new ID\n",
    "        context_counter += 1\n",
    "        context_id = f\"context_{context_counter}\"  # Unique identifier for this context\n",
    "        unique_contexts[context_text] = context_id\n",
    "    \n",
    "    # Map question ID to context ID\n",
    "    question_to_context_id[question_id] = context_id\n",
    "\n",
    "print(f\"\\n Found {len(unique_contexts)} unique contexts\")\n",
    "print(f\"  Total questions: {len(examples)}\")\n",
    "print(f\"  Deduplication: {len(examples) - len(unique_contexts)} duplicate contexts removed\")\n",
    "\n",
    "# Step 2: Create documents only for unique contexts\n",
    "documents = []\n",
    "for context_text, context_id in unique_contexts.items():\n",
    "    doc = Document(\n",
    "        page_content=context_text,  # Full context as one chunk\n",
    "        metadata={\n",
    "            'id': context_id,  # Unique context ID (not question ID)\n",
    "            'unique_context_id': context_id,  # Store for clarity\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "print(f\"\\n Created {len(documents)} documents (one per unique context)\")\n",
    "print(f\"  No chunking performed - each context is a complete chunk\")\n",
    "\n",
    "# Store the mapping for use in evaluation\n",
    "# This will be available in the global scope\n",
    "QUESTION_TO_CONTEXT_ID_MAP = question_to_context_id\n",
    "CONTEXT_ID_TO_DOCS = {doc.metadata['id']: doc for doc in documents}\n",
    "\n",
    "# Examine one document\n",
    "print(\"\\nExample document:\")\n",
    "print(f\"  Content length: {len(documents[0].page_content)} characters\")\n",
    "print(f\"  Unique Context ID: {documents[0].metadata['id']}\")\n",
    "print(f\"\\nExample mapping:\")\n",
    "sample_question_id = list(question_to_context_id.keys())[0]\n",
    "sample_context_id = question_to_context_id[sample_question_id]\n",
    "print(f\"  Question ID '{sample_question_id}' -> Context ID '{sample_context_id}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Initialize Embedding Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding model: BAAI/bge-base-en-v1.5\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_293663/3991924242.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Embedding model loaded\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "print(f\"Initializing embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs={'device': DEVICE}\n",
    ")\n",
    "\n",
    "print(\"\\n Embedding model loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create or Load Vector Store\n",
    "\n",
    "Use a different persist directory to avoid conflicts with chunked version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vector store at ./chroma_db_no_chunking...\n",
      "This may take several minutes (embedding all contexts)...\n",
      " Created vector store with 1204 documents\n",
      "\n",
      "Vector store ready for retrieval!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Use a separate directory for non-chunked version\n",
    "VECTOR_STORE_DIR = \"./chroma_db_no_chunking\"\n",
    "\n",
    "# Check if vector store already exists\n",
    "if os.path.exists(VECTOR_STORE_DIR) and os.listdir(VECTOR_STORE_DIR):\n",
    "    print(f\"Loading existing vector store from {VECTOR_STORE_DIR}...\")\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=VECTOR_STORE_DIR,\n",
    "        embedding_function=embedding_model\n",
    "    )\n",
    "    print(f\" Loaded {vectorstore._collection.count()} documents\")\n",
    "else:\n",
    "    print(f\"Creating new vector store at {VECTOR_STORE_DIR}...\")\n",
    "    print(\"This may take several minutes (embedding all contexts)...\")\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=VECTOR_STORE_DIR\n",
    "    )\n",
    "    print(f\" Created vector store with {vectorstore._collection.count()} documents\")\n",
    "\n",
    "print(\"\\nVector store ready for retrieval!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating retriever to fetch top 3 most relevant contexts...\n",
      " Retriever created\n",
      "\n",
      "Testing retrieval with question: 'What is the capital of France?'\n",
      "\n",
      "Retrieved 3 contexts:\n",
      "\n",
      "  Context 1:\n",
      "  Unique ID: context_1066\n",
      "  Content preview: Warsaw (Polish: Warszawa [varˈʂava] ( listen); see also other names) is the capital and largest city of Poland. It stands on the Vistula River in east...\n",
      "\n",
      "  Context 2:\n",
      "  Unique ID: context_1073\n",
      "  Content preview: Warsaw remained the capital of the Polish–Lithuanian Commonwealth until 1796, when it was annexed by the Kingdom of Prussia to become the capital of t...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Creating retriever to fetch top {K_RETRIEVED} most relevant contexts...\")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": K_RETRIEVED}\n",
    ")\n",
    "\n",
    "print(\" Retriever created\")\n",
    "\n",
    "# Test retrieval\n",
    "test_question = \"What is the capital of France?\"\n",
    "print(f\"\\nTesting retrieval with question: '{test_question}'\")\n",
    "\n",
    "retrieved_docs = retriever.invoke(test_question)\n",
    "print(f\"\\nRetrieved {len(retrieved_docs)} contexts:\")\n",
    "for i, doc in enumerate(retrieved_docs[:2], 1):\n",
    "    print(f\"\\n  Context {i}:\")\n",
    "    print(f\"  Unique ID: {doc.metadata.get('id', 'N/A')}\")\n",
    "    print(f\"  Content preview: {doc.page_content[:150]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate Retriever Accuracy (Recall@3)\n",
    "\n",
    "**Evaluation Logic:**\n",
    "- For each question, we have a **golden context** with a unique ID\n",
    "- We retrieve top 3 chunks and get their IDs\n",
    "- **Recall@3 = % of questions where golden context ID is found in top 3 retrieved chunk IDs**\n",
    "\n",
    "Since we're not chunking, the golden context ID will exactly match the chunk ID if the correct context was retrieved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating retriever accuracy (ID-based Recall@3) on 50 examples...\n",
      "\n",
      "Evaluation method:\n",
      "  - Map question ID to unique context ID\n",
      "  - Check if golden context ID appears in top 3 retrieved chunk IDs\n",
      "  - Only unique contexts stored (no duplicates)\n",
      "\n",
      "Answerable questions: 21\n",
      "Unanswerable questions: 29 (excluded from evaluation)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 21/21 [00:00<00:00, 97.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RETRIEVER ACCURACY RESULTS (ID-based Recall@K)\n",
      "============================================================\n",
      "Total Questions:           50\n",
      "Answerable Questions:      21\n",
      "Unanswerable Questions:    29 (excluded)\n",
      "Golden Context Found:      15\n",
      "\n",
      "Recall@3:            0.7143 (71.43%)\n",
      "\n",
      "Interpretation:\n",
      "  - 71.43% of questions had their golden context ID in top 3 retrieved chunk IDs\n",
      "  - Only unique contexts stored - no duplicate contexts\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_retriever_accuracy_id_based(retriever, examples, max_samples=50):\n",
    "    \"\"\"\n",
    "    Evaluate retriever accuracy using ID-based matching.\n",
    "    \n",
    "    Logic:\n",
    "    - Each question maps to a unique context ID via QUESTION_TO_CONTEXT_ID_MAP\n",
    "    - We retrieve top K chunks and check their context IDs\n",
    "    - Success = golden context ID is found in retrieved chunk IDs\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating retriever accuracy (ID-based Recall@{K_RETRIEVED}) on {max_samples} examples...\")\n",
    "    print(f\"\\nEvaluation method:\")\n",
    "    print(f\"  - Map question ID to unique context ID\")\n",
    "    print(f\"  - Check if golden context ID appears in top {K_RETRIEVED} retrieved chunk IDs\")\n",
    "    print(f\"  - Only unique contexts stored (no duplicates)\\n\")\n",
    "    \n",
    "    # Filter to only answerable questions\n",
    "    answerable_examples = [ex for ex in examples[:max_samples] if not ex['is_impossible']]\n",
    "    unanswerable_count = max_samples - len(answerable_examples)\n",
    "    \n",
    "    print(f\"Answerable questions: {len(answerable_examples)}\")\n",
    "    print(f\"Unanswerable questions: {unanswerable_count} (excluded from evaluation)\\n\")\n",
    "    \n",
    "    correct_retrievals = 0\n",
    "    detailed_results = []\n",
    "    \n",
    "    for example in tqdm(answerable_examples, desc=\"Evaluating retrieval\"):\n",
    "        question = example['question']\n",
    "        question_id = example['id']\n",
    "        \n",
    "        golden_context_id = QUESTION_TO_CONTEXT_ID_MAP.get(question_id)\n",
    "        \n",
    "        if golden_context_id is None:\n",
    "            print(f\"Warning: Question ID {question_id} not found in mapping!\")\n",
    "            continue\n",
    "        \n",
    "        retrieved_docs = retriever.invoke(question)\n",
    "        retrieved_context_ids = [doc.metadata.get('id') for doc in retrieved_docs]\n",
    "        \n",
    "        found = False\n",
    "        rank = None\n",
    "        \n",
    "        for rank_idx, retrieved_id in enumerate(retrieved_context_ids, start=1):\n",
    "            if str(retrieved_id) == str(golden_context_id):\n",
    "                found = True\n",
    "                rank = rank_idx\n",
    "                break\n",
    "        \n",
    "        if found:\n",
    "            correct_retrievals += 1\n",
    "        \n",
    "        detailed_results.append({\n",
    "            'question': question,\n",
    "            'question_id': question_id,\n",
    "            'golden_context_id': golden_context_id,\n",
    "            'found': found,\n",
    "            'rank': rank,\n",
    "            'retrieved_context_ids': retrieved_context_ids\n",
    "        })\n",
    "    \n",
    "    recall_at_k = correct_retrievals / len(answerable_examples) if len(answerable_examples) > 0 else 0.0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RETRIEVER ACCURACY RESULTS (ID-based Recall@K)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total Questions:           {max_samples}\")\n",
    "    print(f\"Answerable Questions:      {len(answerable_examples)}\")\n",
    "    print(f\"Unanswerable Questions:    {unanswerable_count} (excluded)\")\n",
    "    print(f\"Golden Context Found:      {correct_retrievals}\")\n",
    "    print(f\"\\nRecall@{K_RETRIEVED}:            {recall_at_k:.4f} ({recall_at_k*100:.2f}%)\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(f\"  - {recall_at_k*100:.2f}% of questions had their golden context ID in top {K_RETRIEVED} retrieved chunk IDs\")\n",
    "    print(f\"  - Only unique contexts stored - no duplicate contexts\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'recall_at_k': recall_at_k,\n",
    "        'correct_retrievals': correct_retrievals,\n",
    "        'total_answerable': len(answerable_examples),\n",
    "        'detailed_results': detailed_results\n",
    "    }\n",
    "\n",
    "# Run retriever evaluation\n",
    "retriever_metrics = evaluate_retriever_accuracy_id_based(retriever, examples, max_samples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Examine Results\n",
    "\n",
    "Let's look at examples where retrieval succeeded and failed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples where golden context WAS retrieved:\n",
      "\n",
      "1. Question: In what country is Normandy located?...\n",
      "   Golden Context ID: context_1\n",
      "   Found at rank: 2\n",
      "   Retrieved IDs: ['context_4', 'context_1', 'context_31']\n",
      "\n",
      "2. Question: When were the Normans in Normandy?...\n",
      "   Golden Context ID: context_1\n",
      "   Found at rank: 1\n",
      "   Retrieved IDs: ['context_1', 'context_17', 'context_2']\n",
      "\n",
      "3. Question: From which countries did the Norse originate?...\n",
      "   Golden Context ID: context_1\n",
      "   Found at rank: 1\n",
      "   Retrieved IDs: ['context_1', 'context_6', 'context_5']\n",
      "\n",
      "4. Question: Who was the Norse leader?...\n",
      "   Golden Context ID: context_1\n",
      "   Found at rank: 1\n",
      "   Retrieved IDs: ['context_1', 'context_20', 'context_6']\n",
      "\n",
      "5. Question: What century did the Normans first gain their separate identity?...\n",
      "   Golden Context ID: context_1\n",
      "   Found at rank: 1\n",
      "   Retrieved IDs: ['context_1', 'context_18', 'context_7']\n",
      "\n",
      "\n",
      "============================================================\n",
      "Examples where golden context was NOT retrieved:\n",
      "\n",
      "1. Question: Who was the duke in the battle of Hastings?...\n",
      "   Golden Context ID: context_2\n",
      "   Retrieved IDs: ['context_17', 'context_16', 'context_20']\n",
      "\n",
      "2. Question: Who ruled the duchy of Normandy...\n",
      "   Golden Context ID: context_2\n",
      "   Retrieved IDs: ['context_17', 'context_4', 'context_1']\n",
      "\n",
      "3. Question: What was the Norman religion?...\n",
      "   Golden Context ID: context_6\n",
      "   Retrieved IDs: ['context_2', 'context_1', 'context_33']\n",
      "\n",
      "4. Question: What part of France were the Normans located?...\n",
      "   Golden Context ID: context_6\n",
      "   Retrieved IDs: ['context_1', 'context_2', 'context_7']\n",
      "\n",
      "5. Question: What was one of the Norman's major exports?...\n",
      "   Golden Context ID: context_7\n",
      "   Retrieved IDs: ['context_2', 'context_1', 'context_36']\n",
      "\n",
      "\n",
      "Summary:\n",
      "  Successfully retrieved: 15 / 21\n",
      "  Failed to retrieve: 6 / 21\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples where golden context WAS retrieved:\\n\")\n",
    "found_examples = [r for r in retriever_metrics['detailed_results'] if r['found']]\n",
    "for i, result in enumerate(found_examples[:5], 1):\n",
    "    print(f\"{i}. Question: {result['question'][:80]}...\")\n",
    "    print(f\"   Golden Context ID: {result['golden_context_id']}\")\n",
    "    print(f\"   Found at rank: {result['rank']}\")\n",
    "    print(f\"   Retrieved IDs: {result['retrieved_context_ids']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Examples where golden context was NOT retrieved:\\n\")\n",
    "not_found_examples = [r for r in retriever_metrics['detailed_results'] if not r['found']]\n",
    "for i, result in enumerate(not_found_examples[:5], 1):\n",
    "    print(f\"{i}. Question: {result['question'][:80]}...\")\n",
    "    print(f\"   Golden Context ID: {result['golden_context_id']}\")\n",
    "    print(f\"   Retrieved IDs: {result['retrieved_context_ids']}\")\n",
    "    print()\n",
    "    \n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Successfully retrieved: {len(found_examples)} / {len(retriever_metrics['detailed_results'])}\")\n",
    "print(f\"  Failed to retrieve: {len(not_found_examples)} / {len(retriever_metrics['detailed_results'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to retrieval_results_no_chunking_20251225_213912.json\n",
      "\n",
      "Results summary:\n",
      "  Recall@3: 0.7143 (71.43%)\n"
     ]
    }
   ],
   "source": [
    "# Prepare results\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'evaluation_method': 'ID-based Recall@K (no chunking)',\n",
    "    'retriever_metrics': {\n",
    "        'recall_at_k': retriever_metrics['recall_at_k'],\n",
    "        'correct_retrievals': retriever_metrics['correct_retrievals'],\n",
    "        'total_answerable': retriever_metrics['total_answerable']\n",
    "    },\n",
    "    'config': {\n",
    "        'embedding_model': EMBEDDING_MODEL,\n",
    "        'llm_model': LLM_MODEL,\n",
    "        'device': DEVICE,\n",
    "        'k_retrieved': K_RETRIEVED,\n",
    "        'chunking': False,\n",
    "        'description': 'Each context = one chunk, evaluation by ID matching'\n",
    "    },\n",
    "    'sample_results': retriever_metrics['detailed_results'][:10]\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "output_file = f\"retrieval_results_no_chunking_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\" Results saved to {output_file}\")\n",
    "print(f\"\\nResults summary:\")\n",
    "print(f\"  Recall@{K_RETRIEVED}: {retriever_metrics['recall_at_k']:.4f} ({retriever_metrics['recall_at_k']*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Differences from Chunked Approach:\n",
    "\n",
    "1. **No Chunking**: Each context is stored as a single, complete document chunk\n",
    "   - Simplifies evaluation: golden context ID = chunk ID\n",
    "   - Easier to understand: did we retrieve the right context?\n",
    "\n",
    "2. **ID-Based Evaluation**: \n",
    "   - Check if golden context ID (unique index) appears in top 3 retrieved chunk IDs\n",
    "   - Direct matching: if IDs match, we retrieved the correct context\n",
    "\n",
    "3. **Advantages**:\n",
    "   - Simpler evaluation logic\n",
    "   - Clear interpretation: % of questions where correct context was in top 3\n",
    "   - No ambiguity from chunking variations\n",
    "\n",
    "4. **Trade-offs**:\n",
    "   - Larger chunks (full contexts) vs smaller chunks (subsections)\n",
    "   - May retrieve entire context even if only a small part is relevant\n",
    "   - Different retrieval characteristics compared to chunked approach\n",
    "\n",
    "### Evaluation Metric:\n",
    "- **Recall@3**: Percentage of questions where golden context ID was found in top 3 retrieved chunk IDs\n",
    "- Since no chunking, this directly measures: \"Did we retrieve the correct context?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Initialize LLM (Ollama)\n",
    "\n",
    "Initialize the language model that will generate answers based on the retrieved context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM: llama3:8b\n",
      "Temperature: 0 (deterministic)\n",
      " LLM initialized\n",
      "\n",
      "Test response: HELLO, RAG PIPELINE!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "print(f\"Initializing LLM: {LLM_MODEL}\")\n",
    "print(f\"Temperature: {TEMPERATURE} (deterministic)\")\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=LLM_MODEL,\n",
    "    base_url=OLLAMA_BASE_URL,\n",
    "    temperature=TEMPERATURE,\n",
    "    num_ctx=4096,   # Context window size\n",
    "    streaming=False  \n",
    ")\n",
    "\n",
    "print(\" LLM initialized\")\n",
    "\n",
    "# Test LLM with a simple prompt\n",
    "test_response = llm.invoke(\"Say 'Hello, RAG pipeline!'\")\n",
    "print(f\"\\nTest response: {test_response.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Create RAG Prompt Template\n",
    "\n",
    "The prompt template structures how we present the context and question to the LLM. This is crucial for getting good answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prompt template created\n",
      "\n",
      "Example formatted prompt:\n",
      "\n",
      "[SYSTEM]\n",
      "You are a question-answering assistant. Answer the question using ONLY the provided context. If the answer cannot be found in the context, respond with 'I don't know' or 'The answer is not available in the provided context.'\n",
      "\n",
      "[HUMAN]\n",
      "Context:\n",
      "Paris is the capital of France. It is located in the north-central part of the country.\n",
      "\n",
      "Question:\n",
      "What is the capital of France?\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a question-answering assistant. \"\n",
    "        \"Answer the question using ONLY the provided context. \"\n",
    "        \"If the answer cannot be found in the context, respond with 'I don't know' \"\n",
    "        \"or 'The answer is not available in the provided context.'\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n",
    "    )\n",
    "])\n",
    "\n",
    "print(\" Prompt template created\")\n",
    "\n",
    "# Show what a formatted prompt looks like\n",
    "example_context = \"Paris is the capital of France. It is located in the north-central part of the country.\"\n",
    "example_question = \"What is the capital of France?\"\n",
    "\n",
    "formatted = prompt.format_messages(context=example_context, question=example_question)\n",
    "print(\"\\nExample formatted prompt:\")\n",
    "for msg in formatted:\n",
    "    print(f\"\\n[{msg.type.upper()}]\")\n",
    "    print(msg.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Build the Complete RAG Chain\n",
    "\n",
    "Now we combine all components into a single chain that:\n",
    "1. Takes a question\n",
    "2. Retrieves relevant documents\n",
    "3. Formats them as context\n",
    "4. Sends to LLM with prompt\n",
    "5. Returns the answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building RAG chain...\n",
      " RAG chain built\n",
      "\n",
      "Chain flow:\n",
      "  Question → Retrieve Contexts → Format Context → LLM → Answer\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from operator import itemgetter\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Format retrieved documents into a single context string.\n",
    "    Each document is a complete context (no chunking).\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return \"No relevant context found.\"\n",
    "    \n",
    "    # Join contexts with clear separators\n",
    "    contexts = [doc.page_content for doc in docs]\n",
    "    return \"\\n\\n---\\n\\n\".join(contexts)\n",
    "\n",
    "print(\"Building RAG chain...\")\n",
    "\n",
    "# The RAG chain pipeline\n",
    "rag_chain = (\n",
    "    # Step 1: Retrieve contexts for the question\n",
    "    {\n",
    "        \"docs\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    # Step 2: Format contexts into a single string\n",
    "    | RunnableLambda(lambda x: {\n",
    "        \"question\": x[\"question\"],\n",
    "        \"context\": format_docs(x[\"docs\"]),\n",
    "    })\n",
    "    # Step 3: Generate answer using LLM\n",
    "    | RunnableLambda(lambda x: {\n",
    "        \"answer\": (\n",
    "            prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        ).invoke({\n",
    "            \"question\": x[\"question\"],\n",
    "            \"context\": x[\"context\"],\n",
    "        })\n",
    "    })\n",
    ")\n",
    "\n",
    "print(\" RAG chain built\")\n",
    "print(\"\\nChain flow:\")\n",
    "print(\"  Question → Retrieve Contexts → Format Context → LLM → Answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Test the RAG Pipeline\n",
    "\n",
    "Let's test the complete pipeline with a real question from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG pipeline with a sample question...\n",
      "\n",
      "Question: Who ruled the duchy of Normandy\n",
      "Question ID: 56dddf4066d3e219004dad60\n",
      "Golden Context ID: context_2\n",
      "Ground Truth Answer: Richard I\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 1: Check Retriever\n",
      "------------------------------------------------------------\n",
      "Retrieved 3 contexts:\n",
      "\n",
      "  [Context 1]\n",
      "  Context ID: context_17\n",
      "  Content preview: In 1066, Duke William II of Normandy conquered England killing King Harold II at the Battle of Hastings. The invading Normans and their descendants replaced the Anglo-Saxons as the ruling class of Eng...\n",
      "\n",
      "  [Context 2]\n",
      "  Context ID: context_4\n",
      "  Content preview: In the course of the 10th century, the initially destructive incursions of Norse war bands into the rivers of France evolved into more permanent encampments that included local women and personal prop...\n",
      "\n",
      "  [Context 3]\n",
      "  Context ID: context_1\n",
      "  Content preview: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "\n",
      " Golden context NOT found in top 3 retrieved contexts\n",
      "\n",
      "------------------------------------------------------------\n",
      "Step 2: Generate Answer with LLM\n",
      "------------------------------------------------------------\n",
      "\n",
      " Pipeline completed in 0.48 seconds\n",
      "\n",
      "Generated Answer:\n",
      "According to the provided context, Rollo was the one who established the Duchy of Normandy by treaty with King Charles III of West Francia in 911.\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Golden context found: No\n",
      "Answer generated: 146 characters\n",
      "Processing time: 0.48 seconds\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Testing RAG pipeline with a sample question...\")\n",
    "\n",
    "# Get a sample question from the dataset\n",
    "test_example = examples[10]\n",
    "test_question = test_example['question']\n",
    "test_question_id = test_example['id']\n",
    "ground_truth_answer = test_example['answers']['text'][0] if test_example['answers']['text'] else \"N/A\"\n",
    "\n",
    "# Map question ID to context ID\n",
    "golden_context_id = QUESTION_TO_CONTEXT_ID_MAP.get(test_question_id)\n",
    "\n",
    "print(f\"\\nQuestion: {test_question}\")\n",
    "print(f\"Question ID: {test_question_id}\")\n",
    "print(f\"Golden Context ID: {golden_context_id}\")\n",
    "print(f\"Ground Truth Answer: {ground_truth_answer}\")\n",
    "\n",
    "# Step 1: Check Retriever\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Step 1: Check Retriever\")\n",
    "print(\"-\"*60)\n",
    "retrieved = retriever.invoke(test_question)\n",
    "print(f\"Retrieved {len(retrieved)} contexts:\")\n",
    "\n",
    "golden_context_found = False\n",
    "golden_context_rank = None\n",
    "\n",
    "for i, doc in enumerate(retrieved, 1):\n",
    "    doc_id = doc.metadata.get('id')\n",
    "    is_golden_context = (str(doc_id).strip() == str(golden_context_id).strip())\n",
    "    \n",
    "    if is_golden_context:\n",
    "        golden_context_found = True\n",
    "        golden_context_rank = i\n",
    "    \n",
    "    marker = \"  GOLDEN CONTEXT\" if is_golden_context else \"\"\n",
    "    print(f\"\\n  [Context {i}]{marker}\")\n",
    "    print(f\"  Context ID: {doc_id}\")\n",
    "    print(f\"  Content preview: {doc.page_content[:200]}...\")\n",
    "\n",
    "if golden_context_found:\n",
    "    print(f\"\\n Golden context found at rank {golden_context_rank}!\")\n",
    "else:\n",
    "    print(f\"\\n Golden context NOT found in top {K_RETRIEVED} retrieved contexts\")\n",
    "\n",
    "# Step 2: Generate Answer with LLM\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Step 2: Generate Answer with LLM\")\n",
    "print(\"-\"*60)\n",
    "start_time = time.time()\n",
    "\n",
    "result = rag_chain.invoke({\"question\": test_question})\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n Pipeline completed in {elapsed:.2f} seconds\")\n",
    "print(f\"\\nGenerated Answer:\")\n",
    "print(f\"{result['answer']}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "if golden_context_found:\n",
    "    print(f\"Golden context found: Yes (rank {golden_context_rank})\")\n",
    "else:\n",
    "    print(f\"Golden context found: No\")\n",
    "print(f\"Answer generated: {len(result['answer'])} characters\")\n",
    "print(f\"Processing time: {elapsed:.2f} seconds\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Calculate answer quality metrics (recall and exact match):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating answer quality metrics...\n",
      "\n",
      "============================================================\n",
      "ANSWER QUALITY METRICS\n",
      "============================================================\n",
      "Recall:     0.0000 (No)\n",
      "Exact Match: 0.0000 (No)\n",
      "\n",
      "Word Matching Details:\n",
      "  Ground truth words (in order): ['richard', 'i']\n",
      "  Generated words: ['according', 'to', 'provided', 'context', 'rollo', 'was', 'one', 'who', 'established', 'duchy', 'of', 'normandy', 'by', 'treaty', 'with', 'king', 'charles', 'iii', 'of', 'west', 'francia', 'in', '911']\n",
      "  Matched words: []\n",
      "  Expected 2 words, found 0 in order\n",
      "  → Recall = 0.0 (words not found in same order)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating answer quality metrics...\")\n",
    "\n",
    "from metrics_extended import recall_ordered, exact_match\n",
    "\n",
    "# Calculate metrics using the result from Step 13\n",
    "recall = recall_ordered(result['answer'], ground_truth_answer)\n",
    "em = exact_match(result['answer'], ground_truth_answer)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ANSWER QUALITY METRICS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Recall:     {recall:.4f} ({'Yes' if recall == 1.0 else 'No'})\")\n",
    "print(f\"Exact Match: {em:.4f} ({'Yes' if em == 1.0 else 'No'})\")\n",
    "\n",
    "# Show word matching details\n",
    "from metrics_extended import normalize_answer\n",
    "normalized_gen = normalize_answer(result['answer'])\n",
    "normalized_gt = normalize_answer(ground_truth_answer)\n",
    "gt_words = normalized_gt.split()\n",
    "gen_words = normalized_gen.split()\n",
    "\n",
    "print(f\"\\nWord Matching Details:\")\n",
    "print(f\"  Ground truth words (in order): {gt_words}\")\n",
    "print(f\"  Generated words: {gen_words}\")\n",
    "\n",
    "# Find if words appear in order\n",
    "gt_idx = 0\n",
    "matched_positions = []\n",
    "for i, gen_word in enumerate(gen_words):\n",
    "    if gt_idx < len(gt_words) and gen_word == gt_words[gt_idx]:\n",
    "        matched_positions.append((i, gen_word))\n",
    "        gt_idx += 1\n",
    "\n",
    "if len(matched_positions) == len(gt_words):\n",
    "    print(f\"  Matched words in order: {[w for _, w in matched_positions]}\")\n",
    "    print(f\"  Positions in generated answer: {[i for i, _ in matched_positions]}\")\n",
    "    print(f\"  → Recall = 1.0 (all {len(gt_words)} word(s) found in order)\")\n",
    "else:\n",
    "    print(f\"  Matched words: {[w for _, w in matched_positions]}\")\n",
    "    print(f\"  Expected {len(gt_words)} words, found {len(matched_positions)} in order\")\n",
    "    print(f\"  → Recall = 0.0 (words not found in same order)\")\n",
    "\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Evaluate RAG Pipeline on Multiple Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RAG pipeline on 10 examples...\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [00:03<00:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation complete!\n",
      "Average time per question: 0.39 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from metrics_extended import recall_ordered, exact_match\n",
    "\n",
    "# Select a subset of examples for evaluation\n",
    "num_samples = 10  # Use 10 examples for quick evaluation\n",
    "eval_examples = examples[:num_samples]\n",
    "\n",
    "print(f\"Evaluating RAG pipeline on {len(eval_examples)} examples...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "predictions = []\n",
    "ground_truths_list = []\n",
    "is_impossible_list = []\n",
    "processing_times = []\n",
    "\n",
    "# Process each example\n",
    "for i, example in enumerate(tqdm(eval_examples, desc=\"Evaluating\")):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get prediction from RAG pipeline\n",
    "        result = rag_chain.invoke({\"question\": example['question']})\n",
    "        prediction = result[\"answer\"]\n",
    "        predictions.append(prediction)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError on example {i}: {e}\")\n",
    "        predictions.append(\"\")\n",
    "    \n",
    "    # Collect ground truth\n",
    "    ground_truths_list.append(example['answers']['text'])\n",
    "    is_impossible_list.append(example['is_impossible'])\n",
    "    \n",
    "    processing_times.append(time.time() - start_time)\n",
    "\n",
    "print(f\"\\n Evaluation complete!\")\n",
    "print(f\"Average time per question: {np.mean(processing_times):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Calculate Aggregate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating aggregate metrics...\n",
      "\n",
      "============================================================\n",
      "AGGREGATE METRICS\n",
      "============================================================\n",
      "Total Examples:        10\n",
      "\n",
      "Recall:\n",
      "  Mean:                0.4000 ± 0.4899\n",
      "  (40.00% of answers contain all ground truth words in order)\n",
      "\n",
      "Exact Match:\n",
      "  Mean:                0.2000 ± 0.4000\n",
      "  (20.00% of answers match exactly)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating aggregate metrics...\")\n",
    "\n",
    "from metrics_extended import recall_ordered, exact_match\n",
    "\n",
    "# Calculate metrics for each prediction\n",
    "recall_scores = []\n",
    "em_scores = []\n",
    "\n",
    "for i, (prediction, ground_truths) in enumerate(zip(predictions, ground_truths_list)):\n",
    "    # Handle multiple ground truth answers - take the best score\n",
    "    best_recall = 0.0\n",
    "    best_em = 0.0\n",
    "    \n",
    "    for gt in ground_truths:\n",
    "        if gt.strip():  # Skip empty answers\n",
    "            recall_score = recall_ordered(prediction, gt)\n",
    "            em_score = exact_match(prediction, gt)\n",
    "            best_recall = max(best_recall, recall_score)\n",
    "            best_em = max(best_em, em_score)\n",
    "    \n",
    "    recall_scores.append(best_recall)\n",
    "    em_scores.append(best_em)\n",
    "\n",
    "# Calculate aggregate metrics\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_em = np.mean(em_scores)\n",
    "std_recall = np.std(recall_scores)\n",
    "std_em = np.std(em_scores)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"AGGREGATE METRICS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total Examples:        {len(eval_examples)}\")\n",
    "print(f\"\\nRecall:\")\n",
    "print(f\"  Mean:                {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "print(f\"  ({mean_recall*100:.2f}% of answers contain all ground truth words in order)\")\n",
    "print(f\"\\nExact Match:\")\n",
    "print(f\"  Mean:                {mean_em:.4f} ± {std_em:.4f}\")\n",
    "print(f\"  ({mean_em*100:.2f}% of answers match exactly)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Store for later use\n",
    "metrics_results = {\n",
    "    'recall': mean_recall,\n",
    "    'exact_match': mean_em,\n",
    "    'std_recall': std_recall,\n",
    "    'std_em': std_em,\n",
    "    'individual_recall': recall_scores,\n",
    "    'individual_em': em_scores\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Examine Individual Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining individual results...\n",
      "\n",
      "============================================================\n",
      "Example 1\n",
      "============================================================\n",
      "Question: In what country is Normandy located?\n",
      "\n",
      "Ground Truth: France\n",
      "Prediction:   France.\n",
      "\n",
      "Metrics:\n",
      "  Recall:     1.0000 (Yes)\n",
      "  Exact Match: 1.0000 (Yes)\n",
      "\n",
      "============================================================\n",
      "Example 2\n",
      "============================================================\n",
      "Question: When were the Normans in Normandy?\n",
      "\n",
      "Ground Truth: 10th and 11th centuries\n",
      "Prediction:   According to the provided context, the Normans gave their name to Normandy, a region in France, in the 10th and 11th centuries.\n",
      "\n",
      "Metrics:\n",
      "  Recall:     1.0000 (Yes)\n",
      "  Exact Match: 0.0000 (No)\n",
      "\n",
      "============================================================\n",
      "Example 3\n",
      "============================================================\n",
      "Question: From which countries did the Norse originate?\n",
      "\n",
      "Ground Truth: Denmark, Iceland and Norway\n",
      "Prediction:   The Norse originated from Denmark, Iceland, Norway.\n",
      "\n",
      "Metrics:\n",
      "  Recall:     0.0000 (No)\n",
      "  Exact Match: 0.0000 (No)\n",
      "\n",
      "============================================================\n",
      "Example 4\n",
      "============================================================\n",
      "Question: Who was the Norse leader?\n",
      "\n",
      "Ground Truth: Rollo\n",
      "Prediction:   Rollo.\n",
      "\n",
      "Metrics:\n",
      "  Recall:     1.0000 (Yes)\n",
      "  Exact Match: 1.0000 (Yes)\n",
      "\n",
      "============================================================\n",
      "Example 5\n",
      "============================================================\n",
      "Question: What century did the Normans first gain their separate identity?\n",
      "\n",
      "Ground Truth: 10th century\n",
      "Prediction:   According to the provided context, the distinct cultural and ethnic identity of the Normans emerged initially in the \"first half of the 10th century\".\n",
      "\n",
      "Metrics:\n",
      "  Recall:     1.0000 (Yes)\n",
      "  Exact Match: 0.0000 (No)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Examining individual results...\\n\")\n",
    "\n",
    "# Show first 5 examples with their scores\n",
    "for i in range(min(5, len(eval_examples))):\n",
    "    example = eval_examples[i]\n",
    "    pred = predictions[i]\n",
    "    gt = ground_truths_list[i][0] if ground_truths_list[i] else \"(unanswerable)\"\n",
    "    recall_score = recall_scores[i]\n",
    "    em_score = em_scores[i]\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"\\nGround Truth: {gt}\")\n",
    "    print(f\"Prediction:   {pred}\")\n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  Recall:     {recall_score:.4f} ({'Yes' if recall_score == 1.0 else 'No'})\")\n",
    "    print(f\"  Exact Match: {em_score:.4f} ({'Yes' if em_score == 1.0 else 'No'})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to rag_evaluation_results_no_chunking_20251225_214103.json\n",
      "\n",
      "Saved metrics include:\n",
      "  - Answer quality (Recall, Exact Match)\n",
      "  - Individual predictions and scores\n"
     ]
    }
   ],
   "source": [
    "# Prepare results dictionary\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'evaluation_method': 'ID-based Recall@K with ordered word recall',\n",
    "    'retriever_metrics': {\n",
    "        'recall_at_k': retriever_metrics['recall_at_k'],\n",
    "        'correct_retrievals': retriever_metrics['correct_retrievals'],\n",
    "        'total_answerable': retriever_metrics['total_answerable']\n",
    "    },\n",
    "    'answer_quality_metrics': {\n",
    "        'recall': metrics_results['recall'],\n",
    "        'exact_match': metrics_results['exact_match'],\n",
    "        'std_recall': metrics_results['std_recall'],\n",
    "        'std_em': metrics_results['std_em']\n",
    "    },\n",
    "    'dataset_stats': {\n",
    "        'total_samples': len(eval_examples),\n",
    "        'answerable_samples': sum(1 for x in is_impossible_list if not x),\n",
    "        'unanswerable_samples': sum(is_impossible_list)\n",
    "    },\n",
    "    'config': {\n",
    "        'embedding_model': EMBEDDING_MODEL,\n",
    "        'llm_model': LLM_MODEL,\n",
    "        'device': DEVICE,\n",
    "        'k_retrieved': K_RETRIEVED,\n",
    "        'chunking': False,\n",
    "        'description': 'Each context = one chunk, evaluation by ID matching'\n",
    "    },\n",
    "    'sample_predictions': [\n",
    "        {\n",
    "            'question': eval_examples[i]['question'],\n",
    "            'prediction': predictions[i],\n",
    "            'ground_truth': ground_truths_list[i][0] if ground_truths_list[i] else None,\n",
    "            'recall': recall_scores[i],\n",
    "            'exact_match': em_scores[i]\n",
    "        }\n",
    "        for i in range(min(10, len(eval_examples)))\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "output_file = f\"rag_evaluation_results_no_chunking_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\" Results saved to {output_file}\")\n",
    "print(f\"\\nSaved metrics include:\")\n",
    "print(f\"  - Answer quality (Recall, Exact Match)\")\n",
    "print(f\"  - Individual predictions and scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19: Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NOTEBOOK SUMMARY\n",
      "============================================================\n",
      "\n",
      "What we accomplished:\n",
      "1.  Set up RAG pipeline without chunking (each context = one chunk)\n",
      "2.  Created unique context IDs and mapped questions to contexts\n",
      "3.  Evaluated retriever accuracy using ID-based matching\n",
      "4.  Built complete RAG chain (retrieval + generation)\n",
      "5.  Tested pipeline on single example\n",
      "6.  Evaluated on multiple examples\n",
      "7.  Calculated recall (ordered words) and exact match metrics\n",
      "\n",
      "Key Results:\n",
      "  Retriever Recall@3: 71.43%\n",
      "  Answer Recall: 40.00%\n",
      "  Exact Match: 20.00%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"NOTEBOOK SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nWhat we accomplished:\")\n",
    "print(\"1.  Set up RAG pipeline without chunking (each context = one chunk)\")\n",
    "print(\"2.  Created unique context IDs and mapped questions to contexts\")\n",
    "print(\"3.  Evaluated retriever accuracy using ID-based matching\")\n",
    "print(\"4.  Built complete RAG chain (retrieval + generation)\")\n",
    "print(\"5.  Tested pipeline on single example\")\n",
    "print(\"6.  Evaluated on multiple examples\")\n",
    "print(\"7.  Calculated recall (ordered words) and exact match metrics\")\n",
    "print(\"\\nKey Results:\")\n",
    "print(f\"  Retriever Recall@{K_RETRIEVED}: {retriever_metrics['recall_at_k']:.2%}\")\n",
    "print(f\"  Answer Recall: {metrics_results['recall']:.2%}\")\n",
    "print(f\"  Exact Match: {metrics_results['exact_match']:.2%}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Next Steps:\n",
    "- Experiment with different `K_RETRIEVED` values (more documents = more context but potentially more noise)\n",
    "- Try different embedding models to improve retrieval accuracy\n",
    "- Adjust the prompt template to improve generation quality\n",
    "- Evaluate on larger datasets\n",
    "- Compare retriever accuracy with final answer quality to identify improvement opportunities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
